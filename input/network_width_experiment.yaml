experiment:
  ensemble:    
    number_of_networks: 30    
    network:
      implementation: NetworkTF #tensorflow 
      hyperparameters:
        #hidden layer is overrideden by the experiment
        layerwidths: [1, 100, 1]
        activation_function: tanh
        output_activation_function: linear
        scale_weighted_sum: one_over_root_n
        training:          
          learning_rate: 0.01         
          loss_function: mean_squared_error
          #numeric_delta: 0.001  #networkNP only
          #method: backprop  #networkNP only
      initial_parameters:      
        #possible options: generate, file (not yet)     
        source: generate                         
        generate:
          random_seed: 10269
          weights:
            distribution: gaussian
            mean: 0
            variance: 1
          biases:
            distribution: gaussian
            mean: 0
            variance: 1
  data:
    #source of data
    #possible options: generate, file (not yet)    
    source: generate
    #only needed if source: generate
    generate:
      input:      
        min: 0
        max: 1      
        size: 100
        #spacing options: linear, logartihmic, random
        spacing: linear
        training_data_ratio: 0.3
      output:
        function: legendre
        parameters:
          coefficients: [0,0,0,1,0,0,0,0,0,0,0]        
        noise:        
          distribution: gaussian
          random_seed: 10313
          mean: 0
          variance: 0      
  steps: 
    source: list #range
    list: [0, 100, 500, 1000, 1500, 2000]   
  graphics:
    training:     
      - graphs: [theory, training_data, ensemble, ensemble_average]
        x_lim: [-0.1,1.1]
        y_lim: [-1.5, 1.5]            
  results:
    loss_threshold: 0
#    observables: [NTK, K]
    analytic_order: O1
     
   


