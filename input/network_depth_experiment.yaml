experiment:
  ensemble:    
    number_of_networks: 30    
    network:
      implementation: NetworkTF #tensorflow 
      hyperparameters:
        #layerwidths are adjusted for width and depth experiments
        layerwidths: [1, 500, 1]
        activation_function: tanh
        output_activation_function: linear
        scale_weighted_sum: one_over_root_n
        training:          
          learning_rate: 0.3         
          loss_function: mean_squared_error
      initial_parameters:  
        source: generate                         
        generate:
          random_seed: 10269
          weights:
            distribution: gaussian
            mean: 0
            variance: 1
          biases:
            distribution: gaussian
            mean: 0
            variance: 1
  data: 
    source: generate
    generate:
      input:      
        min: 0
        max: 1      
        size: 100
        spacing: linear
        training_data_ratio: 0.3
      output:
        #function is adjusted for trainining time experiments
        function: legendre
        parameters:
          coefficients: [0,0,0,1,0,0,0,0,0,0,0]        
        noise:        
          distribution: gaussian
          random_seed: 10313
          mean: 0
          variance: 0
  theory:
    integration_limits: [-5,5]
    diagonal_regularization: 0.0
    eigenvalue_cutoff: 0.0                
  steps: 
    source: range
    #increase stop for training time experiments
    range:
      start: 0
      stop: 100000100
      step: 100    
  graphics:
    training:     
      - graphs: [theory, training_data, ensemble, ensemble_average]
        x_lim: [-0.1,1.1]
        y_lim: [-1.5, 1.5]            
  results:
    #used for training time experiments
    loss_threshold: 0.01
    analytic_order: O1
     
   


